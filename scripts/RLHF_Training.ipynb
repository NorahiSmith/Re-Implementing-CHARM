{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30664a57",
   "metadata": {},
   "source": [
    "Code to train a model based on RLHF pipeline. Takes as input a pairwise preference pandas dataset as a csv with columns \"prompt\", \"chosen\", and \"rejected\". Trains a given RM on the dataset using loss as defined in https://arxiv.org/pdf/2203.02155 (Ouyang et al 2022, Training language models to follow instructions with human feedback).  Runs in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0390280",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0440b590",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3da9f0",
   "metadata": {},
   "source": [
    "Edit Output Paths Here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0483af7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Name of model in transformers library\n",
    "model_name = \"\"\n",
    "# Name of model that was used to create calibrated dataset\n",
    "training_name = \"\"\n",
    "# Directory of untrained model (if applicable)\n",
    "model_dir = \"\"\n",
    "# Directory to load in dataset\n",
    "data_dir = \"\"\n",
    "# Directory to save trained model\n",
    "output_dir = \"\"\n",
    "# Directory to save loss data\n",
    "loss_dir = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f19c84",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load in tokenizer from transformers library\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27b37a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize and format dataset for training\n",
    "def preprocess(sample):\n",
    "    prompt = str(sample[\"prompt\"])\n",
    "    chosen = str(sample[\"chosen\"])\n",
    "    rejected = str(sample[\"rejected\"])\n",
    "\n",
    "    chosen_input = tokenizer(prompt + chosen, truncation=True, padding=\"max_length\", return_tensors=\"pt\", )\n",
    "    rejected_input = tokenizer(prompt + rejected, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "    return {\n",
    "        \"input_ids_chosen\": chosen_input[\"input_ids\"].squeeze(),\n",
    "        \"attention_mask_chosen\": chosen_input[\"attention_mask\"].squeeze(),\n",
    "        \"input_ids_rejected\": rejected_input[\"input_ids\"].squeeze(),\n",
    "        \"attention_mask_rejected\": rejected_input[\"attention_mask\"].squeeze(),\n",
    "    }\n",
    "\n",
    "# Load in dataframe\n",
    "df = pd.read_csv(data_dir)\n",
    "processed = df.apply(preprocess, axis=1)\n",
    "\n",
    "dataset = list(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e03f8bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Custom reward model class. Forward class requires modification for different LLM output pipelines\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, load_dir = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if load_dir is None:\n",
    "          self.model = AutoModelForCausalLM.from_pretrained(\"Ray2333/GRM-Gemma-2B-sftreg\")\n",
    "        else:\n",
    "          self.model = AutoModelForCausalLM.from_pretrained(load_dir)\n",
    "\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "        self.config = self.model.config\n",
    "        self.v_head = nn.Linear(self.model.config.hidden_size, 1)\n",
    "\n",
    "        if load_dir:\n",
    "            v_head_path = f\"{load_dir}/v_head.pt\"\n",
    "            try:\n",
    "                self.v_head.load_state_dict(torch.load(v_head_path, map_location=\"cpu\"))\n",
    "                print(\"Loaded v_head from:\", v_head_path)\n",
    "            except FileNotFoundError:\n",
    "                print(\"No saved v_head found â€” starting from scratch.\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        reward = output.logits.squeeze(-1)\n",
    "        return reward\n",
    "    def save_pretrained(self, save_directory):\n",
    "        self.model.save_pretrained(save_directory)\n",
    "        torch.save(self.v_head.state_dict(), f\"{save_directory}/v_head.pt\")\n",
    "        print(\"Model and v_head saved to\", save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f324c03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Pairwise loss as defined by RLHF pipeline\n",
    "def pairwise_loss(chosen_rewards, rejected_rewards):\n",
    "    return -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68350750",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize reward model and use dataloader to format dataset\n",
    "model = RewardModel().to(\"cuda\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b666195",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Training loop - requires accelerator and/or peft for bigger models\n",
    "losses = []\n",
    "model.train()\n",
    "i = 0\n",
    "for batch in dataloader:\n",
    "\n",
    "    chosen_rewards = model(batch[\"input_ids_chosen\"].to(\"cuda\"), batch[\"attention_mask_chosen\"].to(\"cuda\"))\n",
    "    rejected_rewards = model(batch[\"input_ids_rejected\"].to(\"cuda\"), batch[\"attention_mask_rejected\"].to(\"cuda\"))\n",
    "\n",
    "    loss = pairwise_loss(chosen_rewards, rejected_rewards)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    print(f\"{i}: Loss: {loss.item()}\")\n",
    "    i+=1\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac7cfb9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save models\n",
    "model.save_pretrained(f\"{output_dir}_model\")\n",
    "tokenizer.save_pretrained(\"{output_dir}_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6a1b54",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(f\"Training Loss: {model_name} trained on {training_name}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b839b6b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save loss as pandas dataframe\n",
    "loss_df = pd.DataFrame({\"step\": list(range(len(losses))), \"loss\": losses})\n",
    "loss_df.to_csv(loss_dir, index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
